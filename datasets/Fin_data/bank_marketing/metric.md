# Bank Marketing — TabPFGen 合成与评估

## 数据与划分
- 原始数据：`data.csv`（41,188 行，特征 20 + label）
- 划分：train 80%（32,950 行，label 0/1=29,238/3,712），test 20%（8,238 行，label 0/1=7,310/928），seed=42 分层
- 特征类型：因包含类别字段，采用 category→codes 编码后数值化；整型/浮点由数据分布自动判定并填充

## TabPFGen 参数
- `n_sgld_steps=200`, `sgld_step_size=0.01`, `sgld_noise_scale=0.01`, `device=cuda`, seed=42
- 生成输入：train 80%（经统一编码和缺失填充后）
- 合成输出（均匀按类初始化，生成后裁剪到真实 min/max，整数列取整）：
  - `synthetic_100.csv` (32,950 行，0/1=16,475/16,475)
  - `synthetic_200.csv` (65,900 行，0/1=32,950/32,950)
  - `synthetic_300.csv` (98,850 行，0/1=49,425/49,425)
  - `synthetic_400.csv` (131,800 行，0/1=65,900/65,900)
  - `synthetic_500.csv` (164,750 行，0/1=82,375/82,375) — 为避免显存溢出，使用 20k 分块逐类生成

## 模型与超参
- logistic: StandardScaler + LogisticRegression(max_iter=500, solver='lbfgs', random_state=42)
- xgboost: n_estimators=300, lr=0.05, max_depth=6, subsample=0.9, colsample_bytree=0.9, objective=binary:logistic, tree_method=hist, random_state=42
- lightgbm: n_estimators=400, lr=0.05, subsample=0.9, colsample_bytree=0.9, random_state=42
- catboost: iterations=400, lr=0.05, depth=6, loss=Logloss, random_seed=42, verbose=False
- mlp: StandardScaler + MLP(hidden_layer_sizes=(128, 64), max_iter=500, alpha=1e-4, random_state=42)
- 测试集固定为真实 test 20%；test 编码与 train 合并后统一类别映射

## 测试结果（合成集训练 → 真实 test 测试）
| synthetic size | model | accuracy | precision | recall | f1 | roc_auc | log_loss |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 100% | logistic | 0.8575 | 0.4357 | 0.8976 | 0.5866 | 0.9388 | 0.3663 |
| 100% | xgboost | 0.8454 | 0.4099 | 0.8481 | 0.5527 | 0.9320 | 0.3760 |
| 100% | lightgbm | 0.8525 | 0.4218 | 0.8341 | 0.5603 | 0.9341 | 0.3615 |
| 100% | catboost | 0.8387 | 0.3998 | 0.8621 | 0.5463 | 0.9332 | 0.3617 |
| 100% | mlp | 0.8773 | 0.4689 | 0.6735 | 0.5529 | 0.9159 | 0.9650 |
| 200% | logistic | 0.8571 | 0.4352 | 0.9009 | 0.5869 | 0.9389 | 0.3675 |
| 200% | xgboost | 0.8481 | 0.4143 | 0.8416 | 0.5553 | 0.9341 | 0.3758 |
| 200% | lightgbm | 0.8529 | 0.4245 | 0.8610 | 0.5687 | 0.9360 | 0.3658 |
| 200% | catboost | 0.8387 | 0.4009 | 0.8739 | 0.5496 | 0.9352 | 0.3629 |
| 200% | mlp | 0.8778 | 0.4688 | 0.6401 | 0.5412 | 0.9134 | 0.9493 |
| 300% | logistic | 0.8574 | 0.4357 | 0.9019 | 0.5876 | 0.9388 | 0.3683 |
| 300% | xgboost | 0.8513 | 0.4206 | 0.8481 | 0.5623 | 0.9338 | 0.3612 |
| 300% | lightgbm | 0.8536 | 0.4254 | 0.8545 | 0.5681 | 0.9355 | 0.3547 |
| 300% | catboost | 0.8446 | 0.4117 | 0.8847 | 0.5619 | 0.9356 | 0.3483 |
| 300% | mlp | 0.8973 | 0.5453 | 0.5323 | 0.5387 | 0.9187 | 1.0874 |
| 400% | logistic | 0.8574 | 0.4355 | 0.8987 | 0.5867 | 0.9388 | 0.3677 |
| 400% | xgboost | 0.8479 | 0.4133 | 0.8351 | 0.5530 | 0.9325 | 0.3649 |
| 400% | lightgbm | 0.8545 | 0.4264 | 0.8459 | 0.5670 | 0.9355 | 0.3536 |
| 400% | catboost | 0.8376 | 0.3996 | 0.8793 | 0.5495 | 0.9345 | 0.3610 |
| 400% | mlp | 0.8813 | 0.4780 | 0.5851 | 0.5262 | 0.9146 | 1.1054 |
| 500% | logistic | 0.1126 | 0.1126 | 1.0000 | 0.2025 | 0.5092 | 31.8216 |
| 500% | xgboost | 0.5195 | 0.1095 | 0.4580 | 0.1768 | 0.4965 | 0.7424 |
| 500% | lightgbm | 0.3430 | 0.0652 | 0.3621 | 0.1105 | 0.3256 | 1.7106 |
| 500% | catboost | 0.4209 | 0.1392 | 0.7985 | 0.2370 | 0.6735 | 0.9594 |
| 500% | mlp | 0.4304 | 0.0703 | 0.3319 | 0.1161 | 0.4024 | 18.7795 |

## 真实集基线（train 80% → test 20%，XGBoost）
- accuracy 0.9310, precision 0.6752, recall 0.5578, f1 0.6113, roc_auc 0.9117, log_loss 0.2235  _(独立基线，未列于上表)_

## 备注
- 合成集基于 80% 训练数据生成；test 编码与 train 统一类别映射。  
- 500% 采用 20k 分块逐类生成以规避显存溢出，仍显著影响模型性能（类别平衡导致分布偏移，指标大幅下降）。  
- 低于 400% 的规模，logistic/lightgbm/xgboost 在 AUC≈0.93–0.94，准确率≈0.85–0.86，表现相对稳定。  
- 训练评估过程存在 PyTorch CUDA 配置弃用警告、MLP 收敛警告，可忽略。***
